{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4755fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import contractions\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cc7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in training & testing dataset, and filter out unknown label\n",
    "df = pd.read_csv('C:/Users/admin/Documents/USCDoc/CSCI 544/project_data/train_dataset_mixed.csv',encoding= 'unicode_escape')\n",
    "df.columns = ['contents','status']\n",
    "df[df.isnull().values==True]\n",
    "df = df.loc[(df['status'] == 'real') | (df['status'] == 'fake')]\n",
    "\n",
    "df2 = pd.read_csv('C:/Users/admin/Documents/USCDoc/CSCI 544/project_data/test_dataset_pure.csv',encoding= 'unicode_escape')\n",
    "df2.columns = ['contents','status']\n",
    "df2[df2.isnull().values==True]\n",
    "df2 = df2.loc[(df2['status'] == 'real') | (df2['status'] == 'fake')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c054b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to contract words\n",
    "def contraction_word(input_str):\n",
    "    words = []\n",
    "    for word in input_str.split():\n",
    "        words.append(contractions.fix(word))\n",
    "    ret_str = ' '.join(words)\n",
    "    return ret_str\n",
    "\n",
    "#function to remove stop words\n",
    "def remove_stopwords(input_str):\n",
    "    words = []\n",
    "\n",
    "    for word in input_str.split(' '):\n",
    "        if word not in stop_words:\n",
    "            words.append(word)\n",
    "    ret_str = ' '.join(words)\n",
    "    return ret_str\n",
    "\n",
    "# lemmatize word\n",
    "def lemmatize_words(input_str):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    res = ''\n",
    "    for w in tokenizer.tokenize(input_str):\n",
    "        res = res + lemmatizer.lemmatize(w) + ' '\n",
    "    res = res[:-1]\n",
    "    return res\n",
    "\n",
    "# function for data cleaning in general\n",
    "def data_cleaning(reviews):\n",
    "    reviews_cleaned = []\n",
    "    for review in reviews:\n",
    "        # convert the all reviews into the lower case.\n",
    "        review = review.lower()\n",
    "        # remove the HTML and URLs from the reviews\n",
    "        review = re.sub(r'http\\S+', '', review)\n",
    "        review = re.sub(r'www.\\S+', '', review)\n",
    "        # remove non-alphabetical characters\n",
    "        review = re.sub(\"[^a-z]+\", ' ', review)\n",
    "        # remove extra spaces\n",
    "        review = re.sub(' +', ' ', review)\n",
    "        # perform contractions on the reviews\n",
    "        review = contraction_word(review)\n",
    "        reviews_cleaned.append(review)\n",
    "\n",
    "    reviews2 = []\n",
    "    for review in reviews_cleaned:\n",
    "        reviews2.append(remove_stopwords(review))\n",
    "\n",
    "    reviews3 = []\n",
    "    for review in reviews2:\n",
    "        reviews3.append(lemmatize_words(review))\n",
    "\n",
    "    return reviews3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17914dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat training and testing temporary to tokenize input sentences\n",
    "df_concat =  pd.concat([df, df2])\n",
    "#factorize output value to 0/1, with respect to fake and real\n",
    "df_concat['status_id'] = [0 if i == 'fake' else 1 for i in df_concat['status']]\n",
    "#perform data cleaning\n",
    "clean_review = data_cleaning(df_concat['contents'])\n",
    "df_concat['cut_review'] = clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c38948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21182, 100) (21182, 2)\n",
      "(1790, 100) (1790, 2)\n"
     ]
    }
   ],
   "source": [
    "#select 15000 commonly used word, in this case, upper bound of number of unique words\n",
    "MAX_NB_WORDS = 15000\n",
    "#set the max sequence length, 99.9% of input sentences are smaller than 100 words\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "#set dimension of embedding layers\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#create tokenizer, to crrate dict of word index based on frequence\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df_concat['cut_review'].values)\n",
    "\n",
    "#create sequences based on word index\n",
    "X = tokenizer.texts_to_sequences(df_concat['cut_review'].values)\n",
    "#unify the length of sequences of each input sentence to 100\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#create one-hot matrix for output value\n",
    "Y = pd.get_dummies(df_concat['status_id']).values\n",
    "\n",
    "#split unified data into training and testing dataset based on input size\n",
    "X_train = X[:df.shape[0]]\n",
    "Y_train = Y[:df.shape[0]]\n",
    "X_test = X[df.shape[0]:]\n",
    "Y_test = Y[df.shape[0]:]\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae6f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         1500000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,580,602\n",
      "Trainable params: 1,580,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define model with embedding dimention, dropout rate, dense dimention\n",
    "#optimizer method, and monitor metrics during training\n",
    "model = Sequential()\n",
    "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "model.add(LSTM(100, dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b6d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "265/265 [==============================] - 35s 124ms/step - loss: 0.2040 - accuracy: 0.9163 - val_loss: 0.5981 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "265/265 [==============================] - 32s 119ms/step - loss: 0.0503 - accuracy: 0.9819 - val_loss: 0.6745 - val_accuracy: 0.6870\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28185900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 2s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "#predict the training dataset\n",
    "y_pred = model.predict(X_test)\n",
    "#use argmax to convert prediction result to 0 or 1\n",
    "y_pred = y_pred.argmax(axis = 1)\n",
    "#convert 2D array to 1D array\n",
    "Y_test = Y_test.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ac31906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9167597765363128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.89      0.90       737\n",
      "        real       0.93      0.93      0.93      1053\n",
      "\n",
      "    accuracy                           0.92      1790\n",
      "   macro avg       0.91      0.91      0.91      1790\n",
      "weighted avg       0.92      0.92      0.92      1790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['fake','real']\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print('accuracy %s' % accuracy_score(y_pred, Y_test))\n",
    "print(classification_report(Y_test, y_pred,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.model to save the model for web application use\n",
    "model.save(\"C:\\\\Users\\\\admin\\\\Documents\\\\USCDoc\\\\CSCI 544\\\\project\\\\lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
